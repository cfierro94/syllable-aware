{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTMclass.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": [
        "QEocT4JON0jT",
        "Cw_ExezE7l_m",
        "-7App8eA99N9"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "QEocT4JON0jT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Keras Install"
      ]
    },
    {
      "metadata": {
        "id": "kmLzaTJ5G3eV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install keras --upgrade"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cw_ExezE7l_m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Clone Github"
      ]
    },
    {
      "metadata": {
        "id": "UsbYugXX8R0h",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "#print(os.getcwd())\n",
        "os.chdir('../content')\n",
        "#os.chdir('../')\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OLXRCPkc8VtQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!rm -R syllable-aware/\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wp0l849H8Xjf",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/nlpchile/syllable-aware.git\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tcx19Pci8j3F",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('syllable-aware')\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-7App8eA99N9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Run LSTM.py"
      ]
    },
    {
      "metadata": {
        "id": "w9bWjO5nLwzl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!python3 LSTM.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ECJ7CIJVX6Kz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Setting Seed for Reproducibility"
      ]
    },
    {
      "metadata": {
        "id": "evNANUZShs9A",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "## Setting Seed for Reproducibility\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Setting PYTHONHASHSEED for determinism was not listed anywhere for TensorFlow,\n",
        "# but apparently it is necessary for the Theano backend\n",
        "# (https://github.com/fchollet/keras/issues/850).\n",
        "\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "seed = 0 # must be the same as PYTHONHASHSEED\n",
        "\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# Limit operation to 1 thread for deterministic results.\n",
        "\n",
        "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1\n",
        "                             )\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "tf.set_random_seed(seed)\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)\n",
        "\n",
        "################################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wSOYawRoRj1T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# LSTM CLASS"
      ]
    },
    {
      "metadata": {
        "id": "YiUKqeUmZQCm",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "\n",
        "class Model:\n",
        "  \n",
        "  def __init__(self,\n",
        "               vocab_size,\n",
        "               embedding_dim,\n",
        "               hidden_dim,\n",
        "               input_length,\n",
        "               recurrent_dropout,\n",
        "               dropout,\n",
        "               seed):\n",
        "\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.input_length = input_length\n",
        "    self.recurrent_dropout = recurrent_dropout\n",
        "    self.dropout = dropout\n",
        "    self.seed = seed\n",
        "\n",
        "\n",
        "    self.word_embeddings = keras.layers.Embedding(input_dim = self.vocab_size+1,\n",
        "                                                  output_dim = self.embedding_dim,\n",
        "                                                  input_length = self.input_length,\n",
        "                                                  mask_zero = True)\n",
        "\n",
        "    self.lstm_1 = keras.layers.LSTM(units = self.hidden_dim,\n",
        "                                    recurrent_dropout = self.recurrent_dropout,\n",
        "                                    return_sequences = True,\n",
        "                                    unroll = False,\n",
        "                                    implementation = 2)\n",
        "\n",
        "    self.dropout_1 = keras.layers.Dropout(rate = self.dropout,\n",
        "                                          seed = self.seed)\n",
        "\n",
        "    self.lstm_2 = keras.layers.LSTM(units = self.hidden_dim,\n",
        "                                    recurrent_dropout = self.recurrent_dropout,\n",
        "                                    return_sequences = False,\n",
        "                                    unroll = False,\n",
        "                                    implementation = 2)\n",
        "\n",
        "    self.dense = keras.layers.Dense(units = self.vocab_size,\n",
        "                                    activation = 'softmax')\n",
        "    \n",
        "    \n",
        "  def build(self, optimizer, metrics):   \n",
        "    \n",
        "    self.optimizer = optimizer    \n",
        "    self.metrics = metrics\n",
        "    \n",
        "    # self.learning_rate = learning_rate # (add to forward)\n",
        "    # self.optimizer = keras.optimizers.RMSprop(lr = self.learning_rate)\n",
        "    \n",
        "    \n",
        "    # Build\n",
        "    \n",
        "    self.model = keras.models.Sequential([self.word_embeddings, self.lstm_1, self.dropout_1, self.lstm_2, self.dense])\n",
        "    \n",
        "    self.model.compile(loss = 'categorical_crossentropy',\n",
        "                       optimizer = self.optimizer,\n",
        "                       metrics = self.metrics)\n",
        "  \n",
        "  \n",
        "  def fit(self, generator, epochs, workers):#, callbacks):\n",
        "    \n",
        "    self.g = generator # Object/Instance Generator, containing .generator() and .steps_per_epoch\n",
        "    \n",
        "    self.epochs = epochs\n",
        "    self.workers = workers  \n",
        "    #self.callbacks = callbacks\n",
        "\n",
        "    self.model.fit_generator(generator = self.g.generator(),\n",
        "                             steps_per_epoch = self.g.steps_per_epoch,\n",
        "                             epochs= self.epochs,\n",
        "                             workers = self.workers,\n",
        "                             #callbacks = self.callbacks,\n",
        "                             shuffle = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dP4ExbNQSCxL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Test LSTM Class\n",
        "## fit( ) Method has not been tested yet"
      ]
    },
    {
      "metadata": {
        "id": "iwBxpcI9gX6H",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "\n",
        "\n",
        "## Hyperparameters\n",
        "\n",
        "\n",
        "D = 512\n",
        "\n",
        "T = 1000\n",
        "\n",
        "Lprima = 100\n",
        "\n",
        "recurrent_dropout = 0.3\n",
        "\n",
        "dropout = 0.3\n",
        "\n",
        "seed = 0 # para capa Dropout\n",
        "\n",
        "\n",
        "## Train Generator\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "ind_corpus_train = #\n",
        "\n",
        "#ind_val_tokens =\n",
        "\n",
        "vocab = #\n",
        "\n",
        "\n",
        "## Fit Model\n",
        "\n",
        "epochs = 300\n",
        "\n",
        "workers = 2\n",
        "\n",
        "# https://keras.io/callbacks/\n",
        "#callbacks =\n",
        "\n",
        "\n",
        "## Model\n",
        "\n",
        "\n",
        "model = Model(vocab_size = T,\n",
        "              embedding_dim = D,\n",
        "              hidden_dim = D,\n",
        "              input_length = Lprima,\n",
        "              recurrent_dropout = recurrent_dropout,\n",
        "              dropout = dropout,\n",
        "              seed = seed)\n",
        "\n",
        "\n",
        "optimizer = 'rmsprop' #'adam'\n",
        "\n",
        "metrics = ['top_k_categorical_accuracy', 'categorical_accuracy']\n",
        "\n",
        "\n",
        "model.build(optimizer = optimizer,\n",
        "            metrics = metrics)\n",
        "\n",
        "train_generator = GeneralGenerator(batch_size = batch_size,\n",
        "                                   ind_tokens = ind_corpus_train,\n",
        "                                   voc = vocab,\n",
        "                                   max_len = Lprima)\n",
        "\n",
        "#val_gen = GeneralGenerator(batch_size = batch_size,\n",
        "#                           ind_tokens = ind_val_tokens, #\n",
        "#                           voc = vocab,\n",
        "#                           max_len = Lprima)\n",
        "\n",
        "model.fit(generator = train_generator,\n",
        "          epochs = epochs,\n",
        "          workers = workers)#,\n",
        "          #callbacks = callbacks)\n",
        "\n",
        "\n",
        "## Test\n",
        "\n",
        "\n",
        "print('\\n')\n",
        "print(model.summary())\n",
        "\n",
        "print('\\n')\n",
        "print(model.loss)\n",
        "\n",
        "print('\\n')\n",
        "print(model.optimizer)\n",
        "\n",
        "print('\\n')\n",
        "print(model.metrics)\n",
        "\n",
        "print('\\n')\n",
        "model.get_config()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ISVyYd-Gu_bJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}